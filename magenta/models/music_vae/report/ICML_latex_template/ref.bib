@article{Donahue2017,
abstract = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are 'doubly deep' in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized.},
archivePrefix = {arXiv},
arxivId = {1411.4389},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2599174},
eprint = {1411.4389},
file = {:home/xdroid/papers/1411.4389v3.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer vision,convolutional nets,deep learning,transfer learning},
number = {4},
pages = {677--691},
title = {{Long-Term Recurrent Convolutional Networks for Visual Recognition and Description}},
volume = {39},
year = {2017}
}
@book{Briot2017,
abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
archivePrefix = {arXiv},
arxivId = {1709.01620},
author = {Briot, Jean-Pierre and Hadjeres, Ga{\"{e}}tan and Pachet, Fran{\c{c}}ois-David},
eprint = {1709.01620},
file = {:home/xdroid/papers/1709.01620.pdf:pdf},
isbn = {9783319701622},
title = {{Deep Learning Techniques for Music Generation -- A Survey}},
url = {http://arxiv.org/abs/1709.01620},
year = {2017}
}
@article{Hung2018,
abstract = {Timbre and pitch are the two main perceptual properties of musical sounds. Depending on the target applications, we sometimes prefer to focus on one of them, while reducing the effect of the other. Researchers have managed to hand-craft such timbre-invariant or pitch-invariant features using domain knowledge and signal processing techniques, but it remains difficult to disentangle them in the resulting feature representations. Drawing upon state-of-the-art techniques in representation learning, we propose in this paper two deep convolutional neural network models for learning disentangled representation of musical timbre and pitch. Both models use encoders/decoders and adversarial training to learn music representations, but the second model additionally uses skip connections to deal with the pitch information. As music is an art of time, the two models are supervised by frame-level instrument and pitch labels using a new dataset collected from MuseScore. We compare the result of the two disentangling models with a new evaluation protocol called "timbre crossover", which leads to interesting applications in audio-domain music editing. Via various objective evaluations, we show that the second model can better change the instrumentation of a multi-instrument music piece without much affecting the pitch structure. By disentangling timbre and pitch, we envision that the model can contribute to generating more realistic music audio as well.},
archivePrefix = {arXiv},
arxivId = {1811.03271},
author = {Hung, Yun-Ning and Chen, Yi-An and Yang, Yi-Hsuan},
eprint = {1811.03271},
file = {:home/xdroid/papers/1811.03271.pdf:pdf},
title = {{Learning Disentangled Representations for Timber and Pitch in Music Audio}},
url = {http://arxiv.org/abs/1811.03271},
year = {2018}
}
@article{Kaliakatsos-Papakostas2017,
abstract = {In computational creativity, new concepts can be invented through conceptual blending of two independent conceptual spaces. Inmusic, conceptual blending has been primarily used for analysing relations between musical and extra-musical elements in composed music rather than generating new music. This paper presents a probabilistic melodic harmonisation assistant that employs conceptual blending to combine learned, potentially diverse, harmonic idioms and generate new harmonic spaces that can be used to harmonise melodies given by the user. The key feature of this system is the application of creative conceptual blending to the most common chord transitions (pairs of consecutive chords) of two initial harmonic idioms. The proposed methodology integrates newly created blended chords and transitions in a compound probabilistic harmonic space, that preserves combined characteristics from both initial idioms along with those new chords and transitions within a unified setting. This methodology enables various interestingmusic applications, ranging from problem-solving, e.g. harmonising melodies that include key transpositions, to generative harmonic exploration, e.g. combining majorâ€“ minor harmonic progressions or more extreme idiosyncratic harmonies.},
author = {Kaliakatsos-Papakostas, Maximos and Queiroz, Marcelo and Tsougras, Costas and Cambouropoulos, Emilios},
doi = {10.1080/09298215.2017.1355393},
file = {:home/xdroid/papers/kaliakatsos-papakostas2017.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {Conceptual blending,Harmonic blending,Markov models,Melodic harmonisation},
number = {4},
pages = {305--328},
publisher = {Routledge},
title = {{Conceptual blending of harmonic spaces for creative melodic harmonisation}},
url = {https://doi.org/10.1080/09298215.2017.1355393},
volume = {46},
year = {2017}
}
@article{Roberts2018,
abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online.2.},
archivePrefix = {arXiv},
arxivId = {1803.05428},
author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
eprint = {1803.05428},
file = {:home/xdroid/Documents/M393C53310/proj/pdf-repo/Final-Project-StatML-DataSci-Spring2020.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {6939--6954},
title = {{A hierarchical latent vector model for learning long-term structure in music}},
volume = {10},
year = {2018}
}
