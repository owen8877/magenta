@book{Briot2017,
abstract = {This paper is a survey and an analysis of different ways of using deep learning (deep artificial neural networks) to generate musical content. We propose a methodology based on five dimensions for our analysis: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenge - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and we propose some tentative multidimensional typology. This typology is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and are used to exemplify the various choices of objective, representation, architecture, challenge and strategy. The last section includes some discussion and some prospects.},
archivePrefix = {arXiv},
arxivId = {1709.01620},
author = {Briot, Jean-Pierre and Hadjeres, Ga{\"{e}}tan and Pachet, Fran{\c{c}}ois-David},
eprint = {1709.01620},
file = {:home/xdroid/papers/1709.01620.pdf:pdf},
isbn = {9783319701622},
title = {{Deep Learning Techniques for Music Generation -- A Survey}},
url = {http://arxiv.org/abs/1709.01620},
year = {2017}
}
@article{Donahue2017,
abstract = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are 'doubly deep' in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized.},
archivePrefix = {arXiv},
arxivId = {1411.4389},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2599174},
eprint = {1411.4389},
file = {:home/xdroid/papers/1411.4389v3.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer vision,convolutional nets,deep learning,transfer learning},
number = {4},
pages = {677--691},
title = {{Long-Term Recurrent Convolutional Networks for Visual Recognition and Description}},
volume = {39},
year = {2017}
}
@article{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
eprint = {1609.03499},
file = {:home/xdroid/papers/1609.03499v2.pdf:pdf},
pages = {1--15},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@article{Jaques2017,
abstract = {This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.},
archivePrefix = {arXiv},
arxivId = {1611.02796},
author = {Jaques, Natasha and Gu, Shixiang and Bahdanau, Dzmitry and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Turner, Richard E. and Eck, Douglas},
eprint = {1611.02796},
file = {:home/xdroid/papers/1611.02796.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {2587--2596},
title = {{Sequence tutor: Conservative fine-tuning of sequence generation models with KL-control}},
volume = {4},
year = {2017}
}
@article{Chu2019,
abstract = {We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03477v1},
author = {Chu, Hang and Urtasun, Raquel and Fidler, Sanja},
eprint = {arXiv:1611.03477v1},
file = {:home/xdroid/papers/1611.03477.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings},
pages = {1--9},
title = {{Song from PI: A musically plausible network for pop music generation}},
year = {2019}
}
@article{Lattner2018,
abstract = {We introduce a method for imposing higher-level structure on generated, polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a generative model is combined with gradient descent constraint optimisation to provide further control over the generation process. Among other things, this allows for the use of a “template” piece, from which some structural properties can be extracted, and transferred as constraints to the newly generated material. The sampling process is guided with Simulated Annealing to avoid local optima, and to find solutions that both satisfy the constraints, and are relatively stable with respect to the C-RBM. Results show that with this approach it is possible to control the higher-level self-similarity structure, the meter, and the tonal properties of the resulting musical piece, while preserving its local musical coherence.},
archivePrefix = {arXiv},
arxivId = {1612.04742},
author = {Lattner, Stefan and Grachten, Maarten and Widmer, Gerhard},
doi = {10.5920/jcms.2018.01},
eprint = {1612.04742},
file = {:home/xdroid/papers/1612.04742v2.pdf:pdf},
issn = {23997656},
journal = {Journal of Creative Music Systems},
keywords = {Constrained sampling,Convolutional restricted Boltzmann machine,Music generation,Optimisation},
number = {0},
title = {{Imposing higher-level structure in polyphonic music generation using convolutional restricted Boltzmann machines and constraints}},
volume = {2},
year = {2018}
}
@article{Roy2017,
abstract = {Machine-learning techniques have been recently used with spectacular results to generate artefacts such as music or text. However, these techniques are still unable to capture and generate artefacts that are convincingly structured. In this paper we present an approach to generate structured musical sequences. We introduce a mechanism for sampling efficiently variations of musical sequences. Given a input sequence and a statistical model, this mechanism samples a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism is implemented as an extension of belief propagation, and uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity measure defined by Mongeau and Sankoff. We then show how this mechanism can used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem.},
archivePrefix = {arXiv},
arxivId = {1703.00760},
author = {Roy, Pierre and Papadopoulos, Alexandre and Pachet, Fran{\c{c}}ois},
eprint = {1703.00760},
file = {:home/xdroid/papers/1703.00760.pdf:pdf},
pages = {1--16},
title = {{Sampling Variations of Lead Sheets}},
url = {http://arxiv.org/abs/1703.00760},
year = {2017}
}
@article{Yang2017,
abstract = {Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.},
archivePrefix = {arXiv},
arxivId = {1703.10847},
author = {Yang, Li Chia and Chou, Szu Yu and Yang, Yi Hsuan},
eprint = {1703.10847},
file = {:home/xdroid/papers/1703.10847.pdf:pdf},
isbn = {9789811151798},
journal = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
pages = {324--331},
title = {{Midinet: A convolutional generative adversarial network for symbolic-domain music generation}},
year = {2017}
}
@article{Hadjeres2017,
abstract = {Recurrent Neural Networks (RNNS) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation. This paper introduces a novel architecture called Anticipation-RNN which possesses the assets of the RNN-based generative models while allowing to enforce user-defined positional constraints. We demonstrate its efficiency on the task of generating melodies satisfying positional constraints in the style of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using the Anticipation-RNN is of the same order of complexity than sampling from the traditional RNN model. This fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes.},
archivePrefix = {arXiv},
arxivId = {1709.06404},
author = {Hadjeres, Ga{\"{e}}tan and Nielsen, Frank},
eprint = {1709.06404},
file = {:home/xdroid/papers/1709.06404v1.pdf:pdf},
pages = {1--9},
title = {{Interactive Music Generation with Positional Constraints using Anticipation-RNNs}},
url = {http://arxiv.org/abs/1709.06404},
year = {2017}
}
@article{Brunner2018,
abstract = {We propose a novel approach for the generation of polyphonic music based on LSTMs. We generate music in two steps. First, a chord LSTM predicts a chord progression based on a chord embedding. A second LSTM then generates polyphonic music from the predicted chord progression. The generated music sounds pleasing and harmonic, with only few dissonant notes. It has clear long-Term structure that is similar to what a musician would play during a jam session. We show that our approach is sensible from a music theory perspective by evaluating the learned chord embeddings. Surprisingly, our simple model managed to extract the circle of fifths, an important tool in music theory, from the dataset.},
archivePrefix = {arXiv},
arxivId = {1711.07682},
author = {Brunner, Gino and Wang, Yuyi and Wattenhofer, Roger and Wiesendanger, Jonas},
doi = {10.1109/ICTAI.2017.00085},
eprint = {1711.07682},
file = {:home/xdroid/papers/1711.07682.pdf:pdf},
isbn = {9781538638767},
issn = {10823409},
journal = {Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
keywords = {Circle of fifths,Deep learning,Embedding,Lstm,Midi,Music generation,Music theory,Polyphonic,Representation learning,Rnn},
pages = {519--526},
title = {{JamBot: Music theory aware chord based generation of polyphonic music with LSTMs}},
volume = {2017-November},
year = {2018}
}
@article{Mao2018,
abstract = {Recent advances in deep neural networks have enabled algorithms to compose music that is comparable to music composed by humans. However, few algorithms allow the user to generate music with tunable parameters. The ability to tune properties of generated music will yield more practical benefits for aiding artists, filmmakers, and composers in their creative tasks. In this paper, we introduce DeepJ - an end-to-end generative model that is capable of composing music conditioned on a specific mixture of composer styles. Our innovations include methods to learn musical style and music dynamics. We use our model to demonstrate a simple technique for controlling the style of generated music as a proof of concept. Evaluation of our model using human raters shows that we have improved over the Biaxial LSTM approach.},
archivePrefix = {arXiv},
arxivId = {1801.00887},
author = {Mao, H. H.},
doi = {10.1109/ICSC.2018.00077},
eprint = {1801.00887},
file = {:home/xdroid/papers/1801.00887.pdf:pdf},
isbn = {9781538644072},
journal = {Proceedings - 12th IEEE International Conference on Semantic Computing, ICSC 2018},
keywords = {biaxial,composer,composition,deep,deepj,generation,genre,learning,lstm,music,style},
pages = {377--382},
title = {{DeepJ: Style-Specific Music Generation}},
volume = {2018-January},
year = {2018}
}
@article{Dong2018,
abstract = {It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binary-valued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binary-valued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445.github.io/bmusegan/.},
archivePrefix = {arXiv},
arxivId = {1804.09399},
author = {Dong, Hao Wen and Yang, Yi Hsuan},
eprint = {1804.09399},
file = {:home/xdroid/papers/1804.09399.pdf:pdf},
isbn = {9782954035123},
journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018},
pages = {190--196},
title = {{Convolutional generative adversarial networks with binary neurons for polyphonic music generation}},
year = {2018}
}
@article{Simon2018,
abstract = {Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch, interpolating between measures in a musically meaningful way, and manipulating specific musical attributes. We also introduce chord conditioning, which allows all of these operations to be performed while keeping harmony fixed, and allows chords to be changed while maintaining musical "style". By generating a sequence of measures over a predefined chord progression, our model can produce music with convincing long-term structure. We demonstrate that our latent space model makes it possible to intuitively control and generate musical sequences with rich instrumentation (see https://goo.gl/s2N7dV for generated audio).},
archivePrefix = {arXiv},
arxivId = {1806.00195},
author = {Simon, Ian and Roberts, Adam and Raffel, Colin and Engel, Jesse and Hawthorne, Curtis and Eck, Douglas},
eprint = {1806.00195},
file = {:home/xdroid/papers/1806.00195.pdf:pdf},
title = {{Learning a Latent Space of Multitrack Measures}},
url = {http://arxiv.org/abs/1806.00195},
year = {2018}
}
@article{Berthelot2019,
abstract = {Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can “interpolate”: By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.},
archivePrefix = {arXiv},
arxivId = {1807.07543},
author = {Berthelot, David and Goodfellow, Ian and Raffel, Colin and Roy, Aurko},
eprint = {1807.07543},
file = {:home/xdroid/papers/1807.07543.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
title = {{Understanding and improving interpolation in autoencoders via an adversarial regularizer}},
year = {2019}
}
@article{Huang2020,
abstract = {The task automatic music composition entails generative modeling of music in symbolic formats such as the musical scores. By serializing a score as a sequence of MIDI-like events, recent work has demonstrated that state-of-the-art sequence models with self-attention work nicely for this task, especially for composing music with long-range coherence. In this paper, we show that sequence models can do even better when we improve the way a musical score is converted into events. The new event set, dubbed "REMI" (REvamped MIDI-derived events), provides sequence models a metric context for modeling the rhythmic patterns of music, while allowing for local tempo changes. Moreover, it explicitly sets up a harmonic structure and makes chord progression controllable. It also facilitates coordinating different tracks of a musical piece, such as the piano, bass and drums. With this new approach, we build a Pop Music Transformer that composes Pop piano music with a more plausible rhythmic structure than prior arts do. The code, data and pre-trained model are publicly available.$\backslash$footnote{\{}$\backslash$url{\{}https://github.com/YatingMusic/remi{\}}{\}}},
archivePrefix = {arXiv},
arxivId = {2002.00212},
author = {Huang, Yu-Siang and Yang, Yi-Hsuan},
eprint = {2002.00212},
file = {:home/xdroid/papers/2002.00212.pdf:pdf},
title = {{Pop Music Transformer: Generating Music with Rhythm and Harmony}},
url = {http://arxiv.org/abs/2002.00212},
year = {2020}
}
@article{Wang2019,
abstract = {Music creation is typically composed of two parts: composing the musical score, and then performing the score with instruments to make sounds. While recent work has made much progress in automatic music generation in the symbolic domain, few attempts have been made to build an AI model that can render realistic music audio from musical scores. Directly synthesizing audio with sound sample libraries often leads to mechanical and deadpan results, since musical scores do not contain performance-level information, such as subtle changes in timing and dynamics. Moreover, while the task may sound like a text-to-speech synthesis problem, there are fundamental differences since music audio has rich polyphonic sounds. To build such an AI performer, we propose in this paper a deep convolutional model that learns in an end-to-end manner the score-to-audio mapping between a symbolic representation of music called the pianorolls and an audio representation of music called the spectrograms. The model consists of two subnets: the ContourNet, which uses a U-Net structure to learn the correspondence between pianorolls and spectrograms and to give an initial result; and the TextureNet, which further uses a multi-band residual network to refine the result by adding the spectral texture of overtones and timbre. We train the model to generate music clips of the violin, cello, and flute, with a dataset of moderate size. We also present the result of a user study that shows our model achieves higher mean opinion score (MOS) in naturalness and emotional expressivity than a WaveNet-based model and two off-the-shelf synthesizers. We open our source code at https://github.com/bwang514/PerformanceNet},
archivePrefix = {arXiv},
arxivId = {1811.04357},
author = {Wang, Bryan and Yang, Yi-Hsuan},
doi = {10.1609/aaai.v33i01.33011174},
eprint = {1811.04357},
file = {:home/xdroid/papers/3911-Article Text-6970-1-10-20190702.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
pages = {1174--1181},
title = {{PerformanceNet: Score-to-Audio Music Generation with Multi-Band Convolutional Residual Network}},
volume = {33},
year = {2019}
}
@article{Dieleman2018,
abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
archivePrefix = {arXiv},
arxivId = {1806.10474},
author = {Dieleman, Sander and {Van Den Oord}, A{\"{a}}ron and Simonyan, Karen},
eprint = {1806.10474},
file = {:home/xdroid/papers/8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {7989--7999},
title = {{The challenge of realistic music generation: Modelling raw audio at scale}},
volume = {2018-December},
year = {2018}
}
@article{Briot2017a,
abstract = {In addition to traditional tasks such as prediction, classification and translation, deep learning is receiving growing attention as an approach for music generation, as witnessed by recent research groups such as Magenta at Google and CTRL (Creator Technology Research Lab) at Spotify. The motivation is in using the capacity of deep learning architectures and training techniques to automatically learn musical styles from arbitrary musical corpora and then to generate samples from the estimated distribution. However, a direct application of deep learning to generate content rapidly reaches limits as the generated content tends to mimic the training set without exhibiting true creativity. Moreover, deep learning architectures do not offer direct ways for controlling generation (e.g., imposing some tonality or other arbitrary constraints). Furthermore, deep learning architectures alone are autistic automata which generate music autonomously without human user interaction, far from the objective of interactively assisting musicians to compose and refine music. Issues such as: control, structure, creativity and interactivity are the focus of our analysis. In this paper, we select some limitations of a direct application of deep learning to music generation, analyze why the issues are not fulfilled and how to address them by possible approaches. Various examples of recent systems are cited as examples of promising directions.},
archivePrefix = {arXiv},
arxivId = {1712.04371},
author = {Briot, Jean-Pierre and Pachet, Fran{\c{c}}ois},
doi = {10.1007/s00521-018-3813-6},
eprint = {1712.04371},
file = {:home/xdroid/papers/1712.04371.pdf:pdf},
pages = {1--17},
title = {{Music Generation by Deep Learning - Challenges and Directions}},
url = {http://arxiv.org/abs/1712.04371{\%}0Ahttp://dx.doi.org/10.1007/s00521-018-3813-6},
year = {2017}
}
@article{Dinculescu2019,
author = {Dinculescu, Monica and Engel, Jesse and Roberts, Adam},
file = {:home/xdroid/papers/c667ad30514350d65e9fa591f8b2263a8abcc9fd.pdf:pdf},
number = {NeurIPS},
pages = {33--35},
title = {{MidiMe : Personalizing a MusicVAE model with user data}},
year = {2019}
}
@article{VanDerWeerdt2018,
abstract = {Music has always been used to elevate the mood in movies and poetry, adding emotions which might not have been without the music. Unfortunately only the most musical people are capable of creating music, let alone the appropriate music. This paper proposes a system that takes as input a piece of text, the representation of that text is consequently transformed into the latent space of a VAE capable of generating music. The latent space of the VAE contains representations of songs and the transformed vector can be decoded from it as a song. An experiment was performed to test this system by presenting a text to seven experts, along with two pieces of music from which one was created from the text. On average the music generated from the text was only recognized in half of the examples, but the poems gave significant results in their recognition, showing a relation between the poems and the generated music.},
author = {{Van Der Weerdt}, Roderick and Schlobach, K S},
file = {:home/xdroid/papers/f189374806.pdf:pdf},
title = {{Generating Music from Text: Mapping Embeddings to a VAE's Latent Space}},
url = {https://esc.fnwi.uva.nl/thesis/centraal/files/f189374806.pdf},
year = {2018}
}
@article{Roberts2018,
abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online.2.},
archivePrefix = {arXiv},
arxivId = {1803.05428},
author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
eprint = {1803.05428},
file = {:home/xdroid/Documents/M393C53310/proj/pdf-repo/Final-Project-StatML-DataSci-Spring2020.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {6939--6954},
title = {{A hierarchical latent vector model for learning long-term structure in music}},
volume = {10},
year = {2018}
}
@article{Hung2018,
abstract = {Timbre and pitch are the two main perceptual properties of musical sounds. Depending on the target applications, we sometimes prefer to focus on one of them, while reducing the effect of the other. Researchers have managed to hand-craft such timbre-invariant or pitch-invariant features using domain knowledge and signal processing techniques, but it remains difficult to disentangle them in the resulting feature representations. Drawing upon state-of-the-art techniques in representation learning, we propose in this paper two deep convolutional neural network models for learning disentangled representation of musical timbre and pitch. Both models use encoders/decoders and adversarial training to learn music representations, but the second model additionally uses skip connections to deal with the pitch information. As music is an art of time, the two models are supervised by frame-level instrument and pitch labels using a new dataset collected from MuseScore. We compare the result of the two disentangling models with a new evaluation protocol called "timbre crossover", which leads to interesting applications in audio-domain music editing. Via various objective evaluations, we show that the second model can better change the instrumentation of a multi-instrument music piece without much affecting the pitch structure. By disentangling timbre and pitch, we envision that the model can contribute to generating more realistic music audio as well.},
archivePrefix = {arXiv},
arxivId = {1811.03271},
author = {Hung, Yun-Ning and Chen, Yi-An and Yang, Yi-Hsuan},
eprint = {1811.03271},
file = {:home/xdroid/papers/1811.03271.pdf:pdf},
title = {{Learning Disentangled Representations for Timber and Pitch in Music Audio}},
url = {http://arxiv.org/abs/1811.03271},
year = {2018}
}
@article{Kaliakatsos-Papakostas2017,
abstract = {In computational creativity, new concepts can be invented through conceptual blending of two independent conceptual spaces. Inmusic, conceptual blending has been primarily used for analysing relations between musical and extra-musical elements in composed music rather than generating new music. This paper presents a probabilistic melodic harmonisation assistant that employs conceptual blending to combine learned, potentially diverse, harmonic idioms and generate new harmonic spaces that can be used to harmonise melodies given by the user. The key feature of this system is the application of creative conceptual blending to the most common chord transitions (pairs of consecutive chords) of two initial harmonic idioms. The proposed methodology integrates newly created blended chords and transitions in a compound probabilistic harmonic space, that preserves combined characteristics from both initial idioms along with those new chords and transitions within a unified setting. This methodology enables various interestingmusic applications, ranging from problem-solving, e.g. harmonising melodies that include key transpositions, to generative harmonic exploration, e.g. combining major– minor harmonic progressions or more extreme idiosyncratic harmonies.},
author = {Kaliakatsos-Papakostas, Maximos and Queiroz, Marcelo and Tsougras, Costas and Cambouropoulos, Emilios},
doi = {10.1080/09298215.2017.1355393},
file = {:home/xdroid/papers/kaliakatsos-papakostas2017.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {Conceptual blending,Harmonic blending,Markov models,Melodic harmonisation},
number = {4},
pages = {305--328},
publisher = {Routledge},
title = {{Conceptual blending of harmonic spaces for creative melodic harmonisation}},
url = {https://doi.org/10.1080/09298215.2017.1355393},
volume = {46},
year = {2017}
}
